{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9gjNoib-M4Q",
        "outputId": "e7fe66c7-9065-46f0-acc1-790284d602fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0BGiw2ESzV5"
      },
      "source": [
        "Prepocess and Merge input Alabama Data.\n",
        "\n",
        "**8. Choosing representative survey questions**\n",
        "In order to get a good picture of tobacco use among each group, I chose some representative questions based on files the Alabama YTS 2016-MS codebook and 2016 YTS Questionnaire-final.\n",
        "\n",
        "Cigaretter:\n",
        "\n",
        "CR7: Have you ever tried cigarette smoking, even one or two puffs?\n",
        "  1. Yes.\n",
        "  2. No\n",
        "\n",
        "CR11: How old were you when you first tried cigarette smoking, even one or two puffs?\n",
        "  1. I have never smoked cigarettes, not even one or two puffs.\n",
        "  2. 8 years old or younger.\n",
        "  3. 9 years old.\n",
        "  4. 10 years old.\n",
        "  5. 11 years old.\n",
        "  6. 12 years old.\n",
        "  7. 13 years old.\n",
        "  8. 14 years old.\n",
        "  9. 15 years old or later.\n",
        "\n",
        "CR12: About how many cigarettes have you smoked in your entire life?\n",
        "  1: I have never smoked cigarettes, not even one or two puffs.\n",
        "  2: 1 or more puffs but never a whole cigarette\n",
        "  3: 1 cigarette\n",
        "  4: 2 to 5 cigarettes\n",
        "  5: 6 to 15 cigarettes (about 1/2 a pack total)\n",
        "  6: 16 to 25 cigarettes (about 1 pack total)\n",
        "  7: 26 to 99 cigarettes (more than 1 pack,  but less than 5 packs)\n",
        "  8: 100 or more cigarettes (5 or more packs)\n",
        "\n",
        "CR13: During the past 30 days, on how many days did you smoke cigarettes?\n",
        "  1. 0 days\n",
        "  2. 1-2 days\n",
        "  3. 3-5 days\n",
        "  4. 6-9 days\n",
        "  5. 10-19 days\n",
        "  6. 20 to 29 days\n",
        "  7. all 30 days\n",
        "\n",
        "CR54: Do you want to stop smoking cigarettes for good?\n",
        "  1. I do not smoke now\n",
        "  2. Yes\n",
        "  3. No\n",
        "\n",
        "CR56: During the past 12 months, how many times have you stopped smoking for one day or longer because you were trying to quit smoking cigarettes for good?\n",
        "  1. I did not smoke during the past 12 months\n",
        "  2. I did not try to quit during the past 12 months\n",
        "  3. 1 time\n",
        "  4. 2 times\n",
        "  5. 3 to 5 times\n",
        "  6. 6 to 9 times\n",
        "  7. 10 or more times\n",
        "\n",
        "Cigars:\n",
        "\n",
        "CR21: Have you ever tried smoking cigars, cigarillos, or little cigars, even one or two puffs?\n",
        "  1. yes\n",
        "  2. no\n",
        "\n",
        "CR22:How old were you when you first tried smoking a cigar, cigarillo, or little cigar, even one or two puffs?\n",
        "  1. I have never smoked cigars, cigarillos, or little cigars, not even one or two puffs\n",
        "  2. 8 years old or younger\n",
        "  3. 9 years old\n",
        "  4. 10 years old\n",
        "  5. 11 years old\n",
        "  6. 12 years old\n",
        "  7. 13 years old\n",
        "  8. 14 years old\n",
        "  9. 15 years old\n",
        "  10. 16 years old\n",
        "  11. 17 years old  \n",
        "  12. 18 years old\n",
        "  13. 19 years old or older\n",
        "\n",
        "CR23: During the past 30 days, on how many days did you smoke cigars, cigarillos, or little cigars?\n",
        "  1. 0 days\n",
        "  2. 1-2 days\n",
        "  3. 3-5 days\n",
        "  4. 6-9 days\n",
        "  5. 10-19 days\n",
        "  6. 20 to 29 days\n",
        "  7. all 30 days"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5EhRjxxK8p3"
      },
      "source": [
        "**9. Processed Raw data file**\n",
        "\n",
        "The raw data file is alabama16_6rigions_ms.sas7bdat.cvs, Alabama14_ms.xlsx, Alabama12_ms.xlsx, which are on the same drive.\n",
        "\n",
        "Combining Columns CR4, CR5A to CR5E to merge it to CR5:\n",
        "If a row has a value in \"CR5A\", set the value in \"CR5\" to 1. If a row has a value in \"CR5B\", set the value in \"CR5'' to 2. Because CR5A is Indian based on the file Alabama YTS 2016-MS codebook,it corresponds to the value 1, which means Indian on our race map.\n",
        "\n",
        "Similarly, \"CR5C\" corresponds to 3, \"CR5D\" to 4, and \"CR5E\" to 5.\n",
        "\n",
        "CR4 is a survey question ask the person whether or not is Hispanic. 1 is no. 2,3,4,5 each means specifc Hispanic group. I will set the value in my \"CR5\" to 6, which means Hispanic on my map.\n",
        "\n",
        "If a row has values in more than one of the columns \"CR5A\" to \"CR5E\" or in CR4, set the value in \"CR5\" to 7, which means two races or more on our map. My map is race_mapping = {'Indian': 1, 'Asian': 2, 'Black': 3, 'Hawaii': 4, 'White': 5, 'Hispanic':6, 'Two race': 7}\n",
        "\n",
        "Selecting Specific Columns:\n",
        "\n",
        "From the filtered rows, select the following columns for gender, grade and race:  \"CR2\", \"CR3\", \"CR5\".\n",
        "\n",
        "select the following columns for the survey questions: \"CR7\", \"CR11\", \"CR12\", \"CR13\", \"CR21\", \"CR22\", \"CR23\", \"CR54\", and \"CR56\".\n",
        "\n",
        "\n",
        "I store the filter result into data_2012, data_2014, data_2016."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7deWyGfQsjs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def merge_cr5(row):\n",
        "    cr5_cols = ['CR5A', 'CR5B', 'CR5C', 'CR5D', 'CR5E']\n",
        "    cr5_flags = [1, 2, 3, 4, 5]\n",
        "    cr5_values = [not pd.isna(row[col]) for col in cr5_cols]  # generates a Boolean list indicating whether each column has a value\n",
        "\n",
        "    if sum(cr5_values) > 1:  # 7 represents two or more races\n",
        "        return 7\n",
        "    elif row['CR4'] in [2, 3, 4, 5] and any(cr5_values):\n",
        "        return 7\n",
        "    elif sum(cr5_values) == 1:\n",
        "        return cr5_flags[cr5_values.index(True)]\n",
        "    elif row['CR4'] in [2, 3, 4, 5]:\n",
        "        return 6\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Preprocess 2012 data\n",
        "data_2012 = pd.read_excel('/content/drive/My Drive/Alabama12_ms.xlsx')\n",
        "data_2012 = data_2012.copy()\n",
        "data_2012['CR5'] = data_2012.apply(merge_cr5, axis=1)\n",
        "selected_columns = ['CR2', 'CR3', 'CR5', 'CR7', 'CR11', 'CR12', 'CR13', 'CR21', 'CR22', 'CR23', 'CR54', 'CR56']\n",
        "data_2012 = data_2012.loc[:, selected_columns]\n",
        "\n",
        "data_2014 = pd.read_excel('/content/drive/My Drive/Alabama14_ms.xlsx')\n",
        "data_2014 = data_2014.copy()\n",
        "data_2014['CR5'] = data_2014.apply(merge_cr5, axis=1)\n",
        "data_2014 = data_2014.loc[:, selected_columns]\n",
        "\n",
        "data_2016 = pd.read_csv('/content/drive/My Drive/alabama16_6rigions_ms.sas7bdat.csv')\n",
        "data_2016 = data_2016.copy()\n",
        "data_2016['CR5'] = data_2016.apply(merge_cr5, axis=1)\n",
        "data_2016 = data_2016.loc[:, selected_columns]\n",
        "\n",
        "# Load the 2024 input data\n",
        "inputs_2024 = pd.read_csv('/content/drive/My Drive/2024_sample_data_not_survey.csv')\n",
        "required_columns = ['CR2', 'CR3', 'CR5']\n",
        "# Ensure the 2024 input data columns match the required columns\n",
        "conditions_2024 = inputs_2024.loc[:, required_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX8N5G4PTuot"
      },
      "outputs": [],
      "source": [
        "data_2012[required_columns] = data_2012[required_columns].fillna(0)\n",
        "data_2014[required_columns] = data_2014[required_columns].fillna(0)\n",
        "data_2016[required_columns] = data_2016[required_columns].fillna(0)\n",
        "conditions_2024.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noGW9-G8bGxk"
      },
      "source": [
        "**10. Model Selection and Training**\n",
        "\n",
        "KNN model: a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.\n",
        "\n",
        "Random Rorest Classifier: a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
        "\n",
        "I trained the 2012, 2014, 2016 Alabama data into these models and choose the most optimal parameter in each model to predict 2024 Alabama data , but I don't include year as a feature, because 2024 year is far way from 2016 year. As I look further back in time, the distance between the vectors of the year becomes greater and greater, largely affecting accuracy. Therefore, the input is only CR2, CR3, CR5. And the output is \"CR7\", \"CR11\", \"CR12\", \"CR13\", \"CR21\", \"CR22\", \"CR23\", \"CR54\", and \"CR56\".\n",
        "\n",
        "When I first attempted to train my model, I noticed that the predicted values for the 2024 dataset, such as the column CR26, were all '2'. This prompted me to review each output column in my training set, where I discovered that most values were dominated by a single number, indicating an uneven distribution. To address the underrepresented groups, I decided to use SMOTE. SMOTE (Synthetic Minority Over-sampling Technique) is an oversampling technique for binary or multiclass tasks, designed to handle class imbalance issues. It generates new synthetic samples in the feature space to increase the number of minority class samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpBHaOEcDCY1",
        "outputId": "bc57ac14-ea7b-45b7-f711-87659792da2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training and evaluating models for CR7\n",
            "KNN Best Hyperparameters for CR7: {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n",
            "KNN Validation Accuracy for CR7: 0.7013574660633484\n",
            "RandomForest Best Hyperparameters for CR7: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50}\n",
            "RandomForest Validation Accuracy for CR7: 0.6119909502262444\n",
            "Best Model for CR7: KNN\n",
            "Test Accuracy for CR7: 0.6812080536912751\n",
            "\n",
            "Training and evaluating models for CR11\n",
            "KNN Best Hyperparameters for CR11: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "KNN Validation Accuracy for CR11: 0.7954939341421143\n",
            "RandomForest Best Hyperparameters for CR11: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "RandomForest Validation Accuracy for CR11: 0.231369150779896\n",
            "Best Model for CR11: KNN\n",
            "Test Accuracy for CR11: 0.8124459809853068\n",
            "\n",
            "Training and evaluating models for CR12\n",
            "KNN Best Hyperparameters for CR12: {'metric': 'euclidean', 'n_neighbors': 7, 'weights': 'distance'}\n",
            "KNN Validation Accuracy for CR12: 0.8025974025974026\n",
            "RandomForest Best Hyperparameters for CR12: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "RandomForest Validation Accuracy for CR12: 0.3264069264069264\n",
            "Best Model for CR12: KNN\n",
            "Test Accuracy for CR12: 0.8076588337684943\n",
            "\n",
            "Training and evaluating models for CR13\n",
            "KNN Best Hyperparameters for CR13: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}\n",
            "KNN Validation Accuracy for CR13: 0.9381533101045296\n",
            "RandomForest Best Hyperparameters for CR13: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "RandomForest Validation Accuracy for CR13: 0.5435540069686411\n",
            "Best Model for CR13: KNN\n",
            "Test Accuracy for CR13: 0.9446366782006921\n",
            "\n",
            "Training and evaluating models for CR21\n",
            "KNN Best Hyperparameters for CR21: {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n",
            "KNN Validation Accuracy for CR21: 0.8266438941076003\n",
            "RandomForest Best Hyperparameters for CR21: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "RandomForest Validation Accuracy for CR21: 0.5704526046114432\n",
            "Best Model for CR21: KNN\n",
            "Test Accuracy for CR21: 0.8334756618274979\n",
            "\n",
            "Training and evaluating models for CR22\n",
            "KNN Best Hyperparameters for CR22: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}\n",
            "KNN Validation Accuracy for CR22: 0.8534923339011925\n",
            "RandomForest Best Hyperparameters for CR22: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "RandomForest Validation Accuracy for CR22: 0.14991482112436116\n",
            "Best Model for CR22: KNN\n",
            "Test Accuracy for CR22: 0.8741438356164384\n",
            "\n",
            "Training and evaluating models for CR23\n",
            "KNN Best Hyperparameters for CR23: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "KNN Validation Accuracy for CR23: 0.9487179487179487\n",
            "RandomForest Best Hyperparameters for CR23: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 200}\n",
            "RandomForest Validation Accuracy for CR23: 0.5957264957264957\n",
            "Best Model for CR23: KNN\n",
            "Test Accuracy for CR23: 0.9598976109215017\n",
            "\n",
            "Training and evaluating models for CR54\n",
            "KNN Best Hyperparameters for CR54: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "KNN Validation Accuracy for CR54: 0.8084163898117387\n",
            "RandomForest Best Hyperparameters for CR54: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "RandomForest Validation Accuracy for CR54: 0.4429678848283499\n",
            "Best Model for CR54: KNN\n",
            "Test Accuracy for CR54: 0.8150984682713348\n",
            "\n",
            "Training and evaluating models for CR56\n",
            "KNN Best Hyperparameters for CR56: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}\n",
            "KNN Validation Accuracy for CR56: 0.8820921985815603\n",
            "RandomForest Best Hyperparameters for CR56: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "RandomForest Validation Accuracy for CR56: 0.4015957446808511\n",
            "Best Model for CR56: KNN\n",
            "Test Accuracy for CR56: 0.8938992042440318\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'CR7': KNeighborsClassifier(metric='manhattan', weights='distance'),\n",
              " 'CR11': KNeighborsClassifier(metric='manhattan', n_neighbors=3),\n",
              " 'CR12': KNeighborsClassifier(metric='euclidean', n_neighbors=7, weights='distance'),\n",
              " 'CR13': KNeighborsClassifier(metric='euclidean', n_neighbors=9),\n",
              " 'CR21': KNeighborsClassifier(metric='manhattan', weights='distance'),\n",
              " 'CR22': KNeighborsClassifier(metric='manhattan', n_neighbors=9, weights='distance'),\n",
              " 'CR23': KNeighborsClassifier(metric='euclidean', n_neighbors=3),\n",
              " 'CR54': KNeighborsClassifier(metric='euclidean', n_neighbors=3, weights='distance'),\n",
              " 'CR56': KNeighborsClassifier(metric='euclidean', n_neighbors=9)}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN, RandomOverSampler\n",
        "\n",
        "# Concatenate the datasets\n",
        "train_data = pd.concat([data_2012, data_2014, data_2016], ignore_index=True)\n",
        "\n",
        "# Shuffle the data\n",
        "train_data = train_data.sample(frac=1)\n",
        "\n",
        "# Input and output columns\n",
        "input_cols = ['CR2', 'CR3', 'CR5']\n",
        "output_cols = ['CR7', 'CR11', 'CR12', 'CR13', 'CR21', 'CR22', 'CR23', 'CR54', 'CR56']\n",
        "\n",
        "# Prepare features and labels\n",
        "X = train_data[input_cols]\n",
        "y = train_data[output_cols]\n",
        "\n",
        "# Split the train, validation, and test set by 0.4, 0.3, 0.3\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Define KNN hyperparameter grid\n",
        "knn_param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Define RandomForest hyperparameter grid\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize dictionary to store the best model for each column\n",
        "best_models = {}\n",
        "\n",
        "# Loop through each output column and perform SMOTE, model training, and prediction separately\n",
        "for col in output_cols:\n",
        "    print(f\"\\nTraining and evaluating models for {col}\")\n",
        "\n",
        "    # Extract the current column for training and testing\n",
        "    y_train_col = y_train[col].dropna()  # Drop NaN values in the current column\n",
        "    X_train_col = X_train.loc[y_train_col.index]  # Adjust X_train to match the filtered y_train_col\n",
        "\n",
        "    y_val_col = y_val[col].dropna()\n",
        "    X_val_col = X_val.loc[y_val_col.index]\n",
        "\n",
        "    y_test_col = y_test[col].dropna()\n",
        "    X_test_col = X_test.loc[y_test_col.index]\n",
        "\n",
        "    # Standardize the feature values for the current column\n",
        "    scaler = StandardScaler()\n",
        "    X_train_col_scaled = scaler.fit_transform(X_train_col)\n",
        "    X_val_col_scaled = scaler.transform(X_val_col)\n",
        "    X_test_col_scaled = scaler.transform(X_test_col)\n",
        "\n",
        "    min_samples = y_train_col.value_counts().min()\n",
        "    class_counts = y_train_col.value_counts()\n",
        "    majority_class_size = class_counts.max()\n",
        "\n",
        "    # Adjust the number of samples for each class (keep majority unchanged, increase minority)\n",
        "    sampling_strategy = {}\n",
        "\n",
        "    for cls, count in class_counts.items():\n",
        "        if count < majority_class_size:\n",
        "            # Increase the number of samples for minority classes\n",
        "            sampling_strategy[cls] = majority_class_size\n",
        "        else:\n",
        "            # Keep the majority class the same\n",
        "            sampling_strategy[cls] = count\n",
        "    if min_samples > 1:\n",
        "        smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42, k_neighbors=min(min_samples - 1, 5))\n",
        "        X_smote_resampled, y_smote_resampled = smote.fit_resample(X_train_col_scaled, y_train_col)\n",
        "\n",
        "        # Further oversample the minority class\n",
        "        ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "        X_train_resampled, y_train_resampled = ros.fit_resample(X_smote_resampled, y_smote_resampled)\n",
        "    else:\n",
        "        # Apply RandomOverSampler directly for very small classes\n",
        "        ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "        X_train_resampled, y_train_resampled = ros.fit_resample(X_train_col_scaled, y_train_col)\n",
        "\n",
        "    # Use GridSearchCV to adjust KNN hyperparameter\n",
        "    knn = KNeighborsClassifier()\n",
        "    knn_grid_search = GridSearchCV(knn, knn_param_grid, cv=5, scoring='accuracy')\n",
        "    knn_grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "    print(f\"KNN Best Hyperparameters for {col}: {knn_grid_search.best_params_}\")\n",
        "\n",
        "    # Use the best parameters to train the KNN model\n",
        "    best_knn = knn_grid_search.best_estimator_\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_val_knn_pred = best_knn.predict(X_val_col_scaled)\n",
        "    val_knn_accuracy = accuracy_score(y_val_col, y_val_knn_pred)\n",
        "    print(f\"KNN Validation Accuracy for {col}: {val_knn_accuracy}\")\n",
        "\n",
        "    # Define RandomForest hyperparameter\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    rf_grid_search = GridSearchCV(rf, rf_param_grid, cv=5, scoring='accuracy')\n",
        "    rf_grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "    print(f\"RandomForest Best Hyperparameters for {col}: {rf_grid_search.best_params_}\")\n",
        "\n",
        "    best_rf = rf_grid_search.best_estimator_\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_val_rf_pred = best_rf.predict(X_val_col_scaled)\n",
        "    val_rf_accuracy = accuracy_score(y_val_col, y_val_rf_pred)\n",
        "    print(f\"RandomForest Validation Accuracy for {col}: {val_rf_accuracy}\")\n",
        "\n",
        "    # Choose between RandomForest and KNN based on validation accuracy\n",
        "    if val_knn_accuracy > val_rf_accuracy:\n",
        "        best_model = best_knn\n",
        "        print(f\"Best Model for {col}: KNN\")\n",
        "    else:\n",
        "        best_model = best_rf\n",
        "        print(f\"Best Model for {col}: RandomForest\")\n",
        "\n",
        "    # Save the best model for this column\n",
        "    best_models[col] = best_model\n",
        "\n",
        "    # Performance on the test set\n",
        "    y_test_pred = best_model.predict(X_test_col_scaled)\n",
        "    test_accuracy = accuracy_score(y_test_col, y_test_pred)\n",
        "    print(f\"Test Accuracy for {col}: {test_accuracy}\")\n",
        "\n",
        "# The best_models dictionary now contains the best model for each output column.\n",
        "best_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaRjppDuDLET"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "predicted_df = pd.DataFrame()\n",
        "conditions_2024_scaled = scaler.transform(conditions_2024)\n",
        "# Loop through each output column and predict individually\n",
        "for col in output_cols:\n",
        "\n",
        "    # Get the best model for this column\n",
        "    best_model = best_models[col]\n",
        "\n",
        "    # Predict the column using the 2024 scaled input data\n",
        "    predictions = best_model.predict(conditions_2024_scaled)\n",
        "\n",
        "    # Add the predictions to the DataFrame\n",
        "    predicted_df[col] = predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAmgDQ3ety2_"
      },
      "outputs": [],
      "source": [
        "result = pd.concat([inputs_2024.reset_index(drop=True), predicted_df], axis=1)\n",
        "result.to_csv('classification_2024_data.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}